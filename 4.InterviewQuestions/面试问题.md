# 面试题
### NLP算法工程师面试问题与答案

#### 1. **NLP有哪些常见任务？**
   - **答案**：文本分类、情感分析、机器翻译、命名实体识别（NER）、问答系统、文本摘要、信息抽取、语言模型、文本生成、语义理解、词性标注（POS）、句法分析、文本聚类、文本相似度计算等。

---

#### 2. **介绍文本表征的方法（词向量）**
   - **答案**：
     - **Word2Vec**：基于上下文预测（CBOW/Skip-Gram），生成静态词向量。
     - **GloVe**：利用全局词共现矩阵，结合统计信息生成词向量。
     - **FastText**：引入子词（subword）信息，解决未登录词问题。
     - **ELMo**：基于双向LSTM的动态上下文词向量。
     - **BERT**：基于Transformer的双向预训练模型，生成上下文相关词向量。

---

#### 3. **如何生成句向量？**
   - **答案**：
     - **平均池化**：对词向量取平均。
     - **LSTM/RNN**：取最后时刻的隐状态。
     - **预训练模型**：如BERT的`[CLS]`向量或各层输出的平均/最大池化。
     - **SIF加权平均**：根据词频对词向量加权平均。

---

#### 4. **如何计算文本相似度？**
   - **答案**：
     - **传统方法**：余弦相似度、Jaccard系数、TF-IDF加权相似度。
     - **词向量方法**：词向量平均后计算相似度。
     - **预训练模型**：用BERT生成句向量后计算余弦相似度。
     - **深度模型**：Siamese网络、Sentence-BERT。

---

#### 5. **样本不平衡的解决方法？**
   - **答案**：
     - **重采样**：过采样（SMOTE）少数类、欠采样多数类。
     - **调整损失权重**：在损失函数中增加少数类的权重。
     - **数据增强**：回译、替换同义词等。
     - **模型层面**：使用F1-score优化、异常检测、集成学习（如EasyEnsemble）。

---

#### 6. **过拟合的表现及解决方法？**
   - **答案**：
     - **表现**：训练集准确率高，测试集显著下降。
     - **解决**：增加数据量、正则化（L1/L2）、降低模型复杂度、早停（Early Stopping）、Dropout、交叉验证、数据增强。

---

#### 7. **用过jieba分词吗？原理是什么？**
   - **答案**：
     - **原理**：基于前缀词典（Trie树）构建词图，通过动态规划找最大概率路径；未登录词用HMM模型识别（如数字、英文、人名等）。
     - **分词模式**：精确模式、全模式、搜索引擎模式。

---

#### 8. **命名实体识别（NER）的方法及特点？**
   - **答案**：
     - **规则方法**：依赖词典和正则，准确率高但泛化差。
     - **CRF**：利用上下文标签转移概率，适合序列标注。
     - **BiLSTM-CRF**：结合双向LSTM捕捉长距离依赖和CRF的标签约束。
     - **BERT**：预训练模型微调，效果最优但计算量大。

---

#### 10. **RNN/LSTM的特点及区别？**
   - **答案**：
     - **RNN**：通过循环结构处理序列，但存在梯度消失/爆炸问题。
     - **LSTM**：引入门控机制（遗忘门、输入门、输出门），缓解梯度问题，能捕捉长距离依赖。

---

#### 11. **re.match()和re.search()的区别？**
   - **答案**：
     - **re.match()**：从字符串**开头**匹配，若开头不匹配则返回`None`。
     - **re.search()**：扫描整个字符串，返回第一个匹配结果。

---

#### 12. **ELMo、GPT、BERT的区别？**
   - **答案**：
     - **ELMo**：双向LSTM，动态词向量，任务特定层需单独训练。
     - **GPT**：单向Transformer（自回归），生成任务表现好。
     - **BERT**：双向Transformer，掩码语言模型（MLM），更适合下游任务微调。

---

#### 13. **BERT的原理及应用？**
   - **答案**：
     - **原理**：基于Transformer编码器，通过MLM（掩码预测）和NSP（下一句预测）预训练。
     - **应用**：文本分类、NER、问答等任务，通过微调适配下游任务。

---

#### 14. **Word2Vec与FastText的区别？**
   - **答案**：
     - **Word2Vec**：词级别训练，无法处理未登录词。
     - **FastText**：引入子词（n-gram），可生成未登录词的向量，适合形态丰富的语言（如中文、德语）。

---

#### 15. **LSTM和Transformer的时间复杂度？**
   - **答案**：
     - **LSTM**：O(n·d²)，n为序列长度，d为隐藏层维度。
     - **Transformer**：O(n²·d)，自注意力机制导致序列长度平方复杂度。

---

#### 16. **知识提取的内容是什么？**
   - **答案**：从非结构化文本中提取结构化知识，如实体、关系、属性等（如人物-职业、公司-总部）。

---

#### 17. **知识图谱的构建流程？**
   - **答案**：
     1. **数据获取**：爬虫、公开数据集。
     2. **信息抽取**：实体识别、关系抽取、属性抽取。
     3. **知识融合**：对齐/消歧（如实体链接）。
     4. **存储**：图数据库（Neo4j）、三元组存储（RDF）。

---

#### 18. **百科知识图谱的项目背景与数据获取？**
   - **答案**：
     - **背景**：结构化百科知识，支持智能搜索、问答。
     - **数据获取**：爬取百科页面（如维基百科、百度百科），标注实体类型（人物、地点等）及关系，数据格式通常为BIOES标注。

---

#### 19. **开放式关系抽取的意义与数据标注？**
   - **答案**：
     - **意义**：传统方法限定固定关系类型，开放式可发现新关系。
     - **标注**：人工标注句子中的主谓宾三元组，或远程监督（利用知识库对齐文本）。

---

#### 20. **实体类型与关系类型？**
   - **答案**：
     - **实体类型**：人物、地点、组织、时间等。
     - **关系类型**：开放式抽取中关系不固定（如“出生于”“就职于”），需通过聚类或规则归一化。

---

#### 21. **为何选择LSTM+CRF而非SOTA模型？**
   - **答案**：
     - **原因**：LSTM+CRF在小数据集上表现稳定，训练成本低；新模型（如BERT）需大量数据且计算资源高，可能未显著提升效果。

---

#### 22. **LSTM原理？**
   - **答案**：通过遗忘门、输入门、输出门控制信息流，解决梯度消失，公式如下：
     - 遗忘门：
        ```math
        f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
        ```
     - 输入门：
        ```math
        i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i
        ``` 
     - 输出门：
        ```math
        o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o
        ```
---

#### 23. **CRF原理？**
   - **答案**：建模标签序列的联合概率，利用转移矩阵（标签间转移得分）和状态矩阵（标签与观测的匹配得分），通过维特比算法解码最优路径。

---

#### 24. **BERT多头注意力原理？**
   - **答案**：将输入向量拆分为多个头（如12头），每个头独立计算注意力，最后拼接结果，增强模型捕捉不同语义子空间的能力。

---

#### 25. **如何为已训练的NER模型新增标签？**
   - **答案**：在模型最后一层扩展标签维度，用少量新标签数据微调，同时冻结部分底层参数防止灾难性遗忘。

---

#### 26. **情感分析的业务内容？**
   - **答案**：分析用户评论/社交媒体的情感倾向（正面/负面/中性），用于产品反馈、舆情监控等。

---

#### 27. **词向量训练工具？**
   - **答案**：常用工具包括Gensim（Word2Vec/FastText）、Hugging Face Transformers（BERT）、FastText库。

---

#### 28. **LSTM后接池化操作？**
   - **答案**：对LSTM输出序列进行最大池化或平均池化，提取全局特征，常用于文本分类（如接全连接层）。

---

#### 29. **使用FastText的理由及工具包？**
   - **答案**：
     - **理由**：支持子词、训练快、适合未登录词。
     - **工具包**：官方FastText库或Gensim。

---

#### 30. **数据不均衡的解决方法？**
   - 同问题5。

---

#### 42. **为何用LSTM？**
   - **答案**：LSTM能捕捉长距离依赖，解决RNN的梯度消失问题，适合序列建模任务（如文本、时序数据）。

---

#### 45. **Transformer简介？**
   - **答案**：基于自注意力机制，无需循环结构，并行计算高效；核心组件为多头注意力、前馈网络、残差连接和层归一化。

---

#### 46. **Bert大火的原因？**
   - **答案**：双向上下文建模能力强、通用性强（通过微调适配多种任务）、预训练+微调范式成为NLP新标准。

---

#### 47. **KNN及KD树优化？**
   - **答案**：
     - **KNN**：根据k个最近邻的类别投票分类。
     - **KD树**：对数据空间递归划分，加速近邻搜索，复杂度从O(n)降至O(log n)。

---

#### 48. **Self-Attention与Attention的区别？**
   - **答案**：
     - **Attention**：关注源序列与目标序列的关系（如机器翻译中Decoder对Encoder的注意力）。
     - **Self-Attention**：序列内部元素间计算注意力，捕捉长距离依赖。

---

#### 49. **为何自训练词向量？**
   - **答案**：领域适配（如医疗、金融术语未在通用预训练词向量中出现）、控制词表大小（减少冗余）。

---

#### 52. **预训练语言模型了解哪些？**
   - **答案**：BERT（MLM+NSP）、ERNIE（融入知识图谱）、XLNet（排列语言模型）、ALBert（参数共享）、RoBERTa（优化训练策略）。

---

#### 53. **Bert的Mask策略为何不全Mask？**
   - **答案**：防止模型过度依赖被掩盖词，同时模拟真实场景中的噪声（如部分词未被掩盖或错误替换），增强鲁棒性。

---

#### 55. **情感分析中聚类算法的输入？**
   - **答案**：文本的向量表示（如句向量、词向量平均），用于无监督聚类（如K-means）发现潜在情感模式。

---

#### 56. **词向量到句向量的计算方式？**
   - 同问题3。

---

#### 58. **过拟合时的操作？**
   - 同问题6。

---

#### 59. **Dropout和正则化防过拟合的原理？**
   - **答案**：
     - **Dropout**：随机丢弃神经元，迫使网络不依赖特定节点，增强泛化。
     - **正则化**：约束权重值（L1稀疏化、L2平滑权重），降低模型复杂度。

---

#### 63. **相似度匹配的粗选+精选思路？**
   - **答案**：
     - **粗选**：快速算法（如BM25、TF-IDF）或倒排索引筛选候选集。
     - **精选**：深度模型（BERT、Sentence-BERT）计算语义相似度。

---

#### 64. **从疫情报道中提取确诊人数最多的国家？**
   - **答案**：
     1. **实体识别**：提取国家名称和确诊人数（正则或NER模型）。
     2. **数据清洗**：统一单位（如“万”转数字）。
     3. **排序**：按确诊数排序，取最大值对应的国家。

---

#### 73. **BERT简介？**
   - **答案**：基于Transformer的双向预训练模型，通过MLM和NSP任务学习上下文表示，微调后适配多种NLP任务。

---

整理完毕，建议根据实际项目经验补充具体案例。