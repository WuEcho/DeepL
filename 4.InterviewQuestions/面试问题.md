# 面试题
### NLP算法工程师面试问题与答案

#### 1. **NLP有哪些常见任务？**
   - **答案**：文本分类、情感分析、机器翻译、命名实体识别（NER）、问答系统、文本摘要、信息抽取、语言模型、文本生成、语义理解、词性标注（POS）、句法分析、文本聚类、文本相似度计算等。

---

#### 2. **介绍文本表征的方法（词向量）**
   - **答案**：
     - **Word2Vec**：基于上下文预测（CBOW/Skip-Gram），生成静态词向量。
     - **GloVe**：利用全局词共现矩阵，结合统计信息生成词向量。
     - **FastText**：引入子词（subword）信息，解决未登录词问题。
     - **ELMo**：基于双向LSTM的动态上下文词向量。
     - **BERT**：基于Transformer的双向预训练模型，生成上下文相关词向量。

---

#### 3. **如何生成句向量？**
   - **答案**：
     - **平均池化**：对词向量取平均。
     - **LSTM/RNN**：取最后时刻的隐状态。
     - **预训练模型**：如BERT的`[CLS]`向量或各层输出的平均/最大池化。
     - **SIF加权平均**：根据词频对词向量加权平均。

---

#### 4. **如何计算文本相似度？**
   - **答案**：
     - **传统方法**：余弦相似度、Jaccard系数、TF-IDF加权相似度。
     - **词向量方法**：词向量平均后计算相似度。
     - **预训练模型**：用BERT生成句向量后计算余弦相似度。
     - **深度模型**：Siamese网络、Sentence-BERT。

---

#### 5. **样本不平衡的解决方法？**
   - **答案**：
     - **重采样**：过采样（SMOTE）少数类、欠采样多数类。
     - **调整损失权重**：在损失函数中增加少数类的权重。
     - **数据增强**：回译、替换同义词等。
     - **模型层面**：使用F1-score优化、异常检测、集成学习（如EasyEnsemble）。

---

#### 6. **过拟合的表现及解决方法？**
   - **答案**：
     - **表现**：训练集准确率高，测试集显著下降。
     - **解决**：增加数据量、正则化（L1/L2）、降低模型复杂度、早停（Early Stopping）、Dropout、交叉验证、数据增强。

---

#### 7. **用过jieba分词吗？原理是什么？**
   - **答案**：
     - **原理**：基于前缀词典（Trie树）构建词图，通过动态规划找最大概率路径；未登录词用HMM模型识别（如数字、英文、人名等）。
     - **分词模式**：精确模式、全模式、搜索引擎模式。

---

#### 8. **命名实体识别（NER）的方法及特点？**
   - **答案**：
     - **规则方法**：依赖词典和正则，准确率高但泛化差。
     - **CRF**：利用上下文标签转移概率，适合序列标注。
     - **BiLSTM-CRF**：结合双向LSTM捕捉长距离依赖和CRF的标签约束。
     - **BERT**：预训练模型微调，效果最优但计算量大。

---

#### 10. **RNN/LSTM的特点及区别？**
   - **答案**：
     - **RNN**：通过循环结构处理序列，但存在梯度消失/爆炸问题。
     - **LSTM**：引入门控机制（遗忘门、输入门、输出门），缓解梯度问题，能捕捉长距离依赖。

---

#### 11. **re.match()和re.search()的区别？**
   - **答案**：
     - **re.match()**：从字符串**开头**匹配，若开头不匹配则返回`None`。
     - **re.search()**：扫描整个字符串，返回第一个匹配结果。

---

#### 12. **ELMo、GPT、BERT的区别？**
   - **答案**：
     - **ELMo**：双向LSTM，动态词向量，任务特定层需单独训练。
     - **GPT**：单向Transformer（自回归），生成任务表现好。
     - **BERT**：双向Transformer，掩码语言模型（MLM），更适合下游任务微调。

---

#### 13. **BERT的原理及应用？**
   - **答案**：
     - **原理**：基于Transformer编码器，通过MLM（掩码预测）和NSP（下一句预测）预训练。
     - **应用**：文本分类、NER、问答等任务，通过微调适配下游任务。

---

#### 14. **Word2Vec与FastText的区别？**
   - **答案**：
     - **Word2Vec**：词级别训练，无法处理未登录词。
     - **FastText**：引入子词（n-gram），可生成未登录词的向量，适合形态丰富的语言（如中文、德语）。

---

#### 15. **LSTM和Transformer的时间复杂度？**
   - **答案**：
     - **LSTM**：O(n·d²)，n为序列长度，d为隐藏层维度。
     - **Transformer**：O(n²·d)，自注意力机制导致序列长度平方复杂度。

---

#### 16. **知识提取的内容是什么？**
   - **答案**：从非结构化文本中提取结构化知识，如实体、关系、属性等（如人物-职业、公司-总部）。

---

#### 17. **知识图谱的构建流程？**
   - **答案**：
     1. **数据获取**：爬虫、公开数据集。
     2. **信息抽取**：实体识别、关系抽取、属性抽取。
     3. **知识融合**：对齐/消歧（如实体链接）。
     4. **存储**：图数据库（Neo4j）、三元组存储（RDF）。

---

#### 18. **百科知识图谱的项目背景与数据获取？**
   - **答案**：
     - **背景**：结构化百科知识，支持智能搜索、问答。
     - **数据获取**：爬取百科页面（如维基百科、百度百科），标注实体类型（人物、地点等）及关系，数据格式通常为BIOES标注。

---

#### 19. **开放式关系抽取的意义与数据标注？**
   - **答案**：
     - **意义**：传统方法限定固定关系类型，开放式可发现新关系。
     - **标注**：人工标注句子中的主谓宾三元组，或远程监督（利用知识库对齐文本）。

---

#### 20. **实体类型与关系类型？**
   - **答案**：
     - **实体类型**：人物、地点、组织、时间等。
     - **关系类型**：开放式抽取中关系不固定（如“出生于”“就职于”），需通过聚类或规则归一化。

---

#### 21. **为何选择LSTM+CRF而非SOTA模型？**
   - **答案**：
     - **原因**：LSTM+CRF在小数据集上表现稳定，训练成本低；新模型（如BERT）需大量数据且计算资源高，可能未显著提升效果。

---

#### 22. **LSTM原理？**
   - **答案**：通过遗忘门、输入门、输出门控制信息流，解决梯度消失，公式如下：
     - 遗忘门：

        $ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $

     - 输入门：
        
        $ i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i $
 
     - 输出门：
        
        $ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o $

---

#### 23. **CRF原理？**
   - **答案**：建模标签序列的联合概率，利用转移矩阵（标签间转移得分）和状态矩阵（标签与观测的匹配得分），通过维特比算法解码最优路径。

---

#### 24. **BERT多头注意力原理？**
   - **答案**：将输入向量拆分为多个头（如12头），每个头独立计算注意力，最后拼接结果，增强模型捕捉不同语义子空间的能力。

---

#### 25. **如何为已训练的NER模型新增标签？**
   - **答案**：在模型最后一层扩展标签维度，用少量新标签数据微调，同时冻结部分底层参数防止灾难性遗忘。

---

#### 26. **情感分析的业务内容？**
   - **答案**：分析用户评论/社交媒体的情感倾向（正面/负面/中性），用于产品反馈、舆情监控等。

---

#### 27. **词向量训练工具？**
   - **答案**：常用工具包括Gensim（Word2Vec/FastText）、Hugging Face Transformers（BERT）、FastText库。

---

#### 28. **LSTM后接池化操作？**
   - **答案**：对LSTM输出序列进行最大池化或平均池化，提取全局特征，常用于文本分类（如接全连接层）。

---

#### 29. **使用FastText的理由及工具包？**
   - **答案**：
     - **理由**：支持子词、训练快、适合未登录词。
     - **工具包**：官方FastText库或Gensim。

---


#### 42. **为何用LSTM？**
   - **答案**：LSTM能捕捉长距离依赖，解决RNN的梯度消失问题，适合序列建模任务（如文本、时序数据）。

---

#### 45. **Transformer简介？**
   - **答案**：基于自注意力机制，无需循环结构，并行计算高效；核心组件为多头注意力、前馈网络、残差连接和层归一化。

---

#### 46. **Bert大火的原因？**
   - **答案**：双向上下文建模能力强、通用性强（通过微调适配多种任务）、预训练+微调范式成为NLP新标准。

---

#### 47. **KNN及KD树优化？**
   - **答案**：
     - **KNN**：根据k个最近邻的类别投票分类。
     - **KD树**：对数据空间递归划分，加速近邻搜索，复杂度从O(n)降至O(log n)。

---

#### 48. **Self-Attention与Attention的区别？**
   - **答案**：
     - **Attention**：关注源序列与目标序列的关系（如机器翻译中Decoder对Encoder的注意力）。
     - **Self-Attention**：序列内部元素间计算注意力，捕捉长距离依赖。

---

#### 49. **为何自训练词向量？**
   - **答案**：领域适配（如医疗、金融术语未在通用预训练词向量中出现）、控制词表大小（减少冗余）。

---

#### 52. **预训练语言模型了解哪些？**
   - **答案**：BERT（MLM+NSP）、ERNIE（融入知识图谱）、XLNet（排列语言模型）、ALBert（参数共享）、RoBERTa（优化训练策略）。

---

#### 53. **Bert的Mask策略为何不全Mask？**
   - **答案**：防止模型过度依赖被掩盖词，同时模拟真实场景中的噪声（如部分词未被掩盖或错误替换），增强鲁棒性。

---

#### 55. **情感分析中聚类算法的输入？**
   - **答案**：文本的向量表示（如句向量、词向量平均），用于无监督聚类（如K-means）发现潜在情感模式。

#### 56. **RAG的搜索模块与排序模块采用了什么模型？**
在RAG（Retrieval-Augmented Generation）系统中，**检索模型**和**重排序模型**是两个关键组件，分别负责**快速召回候选文档**和**精细化排序**。以下是它们的实现方式及典型模型结构：
实际应用中建议：**先用BM25+DPR保证召回广度，再用Cross-Encoder提升精度**。

- 一、检索模型（Retriever）

#### 1. **核心目标**  
快速从大规模文档库（百万级）中召回Top-K（如100-200）相关候选，需平衡**效率**与**初步相关性**。

#### 2. **主流实现方案**
##### **(1) 稀疏检索（Sparse Retrieval）**
- **代表模型**：BM25（词频-逆文档频统计模型）  
- **原理**：  
  基于关键词匹配，计算查询词与文档的词频统计权重。  

```math
\text{BM25}(q,d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{f_{t,d} \cdot (k_1 + 1)}{f_{t,d} + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}
```

- **优点**：无需训练、计算极快（适合冷启动）。  
- **缺点**：无法处理语义相似性（如“计算机”和“电脑”无法关联）。

##### **(2) 密集检索（Dense Retrieval）**
- **代表模型**：DPR（Dense Passage Retrieval）、ANCE  
- **模型结构**：  
  - **双编码器架构**（双塔模型）：  
    - **Query Encoder**：BERT将查询编码为向量 $ q \in \mathbb{R}^d $。  
    - **Document Encoder**：BERT将文档编码为向量 $ d \in \mathbb{R}^d $。  
    - **相似度计算**：余弦相似度 $ \text{sim}(q,d) = \frac{q \cdot d}{\|q\| \|d\|} $。  
  - **训练方法**：对比学习（正样本：相关文档，负样本：随机文档或难负例）。  
  - **损失函数**：  

```math
\mathcal{L} = -\log \frac{e^{\text{sim}(q,d^+)}}{e^{\text{sim}(q,d^+)} + \sum_{d^-} e^{\text{sim}(q,d^-)}}
```

- **优点**：语义匹配能力强。  
- **缺点**：需预训练编码器，索引更新成本高。

##### **(3) 混合检索（Hybrid Retrieval）**
- **方案**：BM25 + 密集检索的混合打分（加权或级联）。  
- **示例**：  
  
```math
\text{Score}(q,d) = \alpha \cdot \text{BM25}(q,d) + (1-\alpha) \cdot \text{Dense}(q,d)
```

#### 3. **工程实现**  
- **索引构建**：  
  - 稀疏检索：使用倒排索引（如Elasticsearch）。  
  - 密集检索：使用向量数据库（如FAISS、HNSW）。  
- **加速技巧**：  
  - 量化（如SQ8）：将float32向量压缩为8-bit整数。  
  - 分段索引（Sharding）：将文档库拆分为多个子索引并行查询。

- 二、重排序模型（Reranker）
#### 1. **核心目标**  
对检索模型召回的Top-K候选（如100条）进行精细化排序，选出Top-N（如5-10条）最相关文档。

#### 2. **主流实现方案**
##### **(1) 交叉编码器（Cross-Encoder）**
- **代表模型**：BERT、RoBERTa  
- **模型结构**：  
  - 输入：将查询`q `和文档`d`拼接为`[CLS] q [SEP] d [SEP]`。  
  - 输出：`[CLS]`位置的向量通过线性层，输出相关性分数 $ s \in \mathbb{R} $。  
  - **结构示意图**：  

```python
    class CrossEncoder(nn.Module):
        def __init__(self, model_name="bert-base"):
            super().__init__()
            self.bert = BertModel.from_pretrained(model_name)
            self.classifier = nn.Linear(768, 1)
        
        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids, attention_mask)
            cls_vector = outputs.last_hidden_state[:, 0, :]
            return self.classifier(cls_vector).squeeze(-1)  # [batch_size]
```
- **训练方法**：  
  - 数据：三元组 $ (q, d^+, d^-) $。  
  - 损失函数：Pairwise Ranking Loss（如Margin Loss）：  
    
```math
\mathcal{L} = \max(0, \epsilon - s(q,d^+) + s(q,d^-))
```
- **优点**：准确率高，能捕捉细粒度交互特征。  
- **缺点**：计算成本高（无法预先计算文档向量）。

##### **(2) 生成式重排序**
- **代表模型**：T5、BART  
- **方法**：  
  - 输入：查询 \( q \) 和文档 \( d \)。  
  - 输出：生成相关性标签（如“相关”或“不相关”）。  
  - **示例提示**：  
    
    ```
    判断问题和文档的相关性：
    问题：如何治疗感冒？
    文档：感冒通常由病毒引起，建议多休息、补充水分。
    答案：相关
    ```
- **优点**：可结合生成能力处理复杂语义。  
- **缺点**：推理速度慢，需生成模型支持。

##### **(3) 轻量级重排序模型**
- **代表模型**：ColBERT、Poly-Encoder  
- **ColBERT原理**：  
  - 将查询和文档分别编码为多个向量（每个Token一个向量）。  
  - 计算查询每个Token与文档所有Token的最大相似度，求和作为总分。  
  - **数学表达**：  
```math
\text{Score}(q,d) = \sum_{t_q \in q} \max_{t_d \in d} \text{sim}(e_{t_q}, e_{t_d})
```
  - **优点**：支持预先编码文档，推理时仅需计算查询向量。  

#### 3. **工程优化**
- **缓存机制**：  
  - 对高频查询的Top-K结果缓存重排序结果。  
- **并行计算**：  
  - 批量处理多个（q,d）对（如一次推理128对）。  
- **动态剪枝**：  
  - 若某文档的中间分数已低于阈值，提前终止计算。

---

- 三、典型RAG流程示例
1. **用户提问**：  
   `Q: 如何解决Python中的内存溢出问题？`

2. **检索阶段**：  
   - BM25召回：Top 200文档（含代码示例、技术博客等）。  
   - 密集检索召回：Top 100文档（语义相关但可能包含冗余）。  
   - 合并去重后得到300候选文档。

3. **重排序阶段**：  
   - 使用BERT Cross-Encoder对300文档排序。  
   - 保留Top 5文档：  
     ```
     1. "使用生成器替代列表处理大数据集..."（Score: 0.92）
     2. "通过gc模块手动回收无用对象..."（Score: 0.87）
     ...
     ```

4. **生成阶段**：  
   - 将Top 5文档与问题输入LLM（如GPT-3），生成最终回答。

---

-  四、模型对比与选型建议
| 模型类型       | 计算速度 | 准确性 | 适用场景                |
|----------------|----------|--------|-------------------------|
| BM25           | ★★★★★    | ★★☆☆☆ | 关键词匹配、冷启动阶段  |
| DPR            | ★★★★☆    | ★★★★☆ | 语义搜索、开放域问答    |
| Cross-Encoder   | ★★☆☆☆    | ★★★★★ | 精细化排序、小规模候选  |
| ColBERT        | ★★★☆☆    | ★★★★☆ | 平衡效率与精度          |

#### 59. **Dropout和正则化防过拟合的原理？**
   - **答案**：
     - **Dropout**：随机丢弃神经元，迫使网络不依赖特定节点，增强泛化。
     - **正则化**：约束权重值（L1稀疏化、L2平滑权重），降低模型复杂度。

---

#### 63. **相似度匹配的粗选+精选思路？**
   - **答案**：
     - **粗选**：快速算法（如BM25、TF-IDF）或倒排索引筛选候选集。
     - **精选**：深度模型（BERT、Sentence-BERT）计算语义相似度。

---

#### 64. **从疫情报道中提取确诊人数最多的国家？**
   - **答案**：
     1. **实体识别**：提取国家名称和确诊人数（正则或NER模型）。
     2. **数据清洗**：统一单位（如“万”转数字）。
     3. **排序**：按确诊数排序，取最大值对应的国家。

---

#### 73. **BERT简介？**
   - **答案**：基于Transformer的双向预训练模型，通过MLM和NSP任务学习上下文表示，微调后适配多种NLP任务。

---

整理完毕，建议根据实际项目经验补充具体案例。

