# 词的向量化

## 向量化向量对于机器学习非常重要，大量的算法都需要基于向量来完成### 文本向量化对于机器来说，字符是没有含义的，只是有区别；只使用字符无法去刻画字与字、词与词、文本与文本之间的关系；文本转化为向量可以更好地刻画文本之间的关系，向量化后，可以启用大量的机器学习算法，具有很高的价值。

文本是由词和字组成的，想将文本转化为向量，首先要能够把词和字转化为向量，所有向量应该有同一维度n，我们可以称这个n维空间是一个语义空间。

我               [0.78029002 0.77010974 0.07479124 0.4106988 ]爱               [0.14092194 0.63690971 0.73774712 0.42768218]北京           [0.95780568 0.51903789 0.76615855 0.6399924 ]天安门       [0.73861383 0.49694373 0.13213538 0.41237077]#### one-hot编码
- 一个1或干个零组成的向量。首先统计一个字表或词表，选出n个字或词，比如：

| 字、词 | 向量            |
|-------|-----------------|
| 今天  | [1, 0, 0, 0, 0] |
| 天气  | [0, 1, 0, 0, 0] |
| 真    | [0, 0, 1, 0, 0] |
| 不错  | [0, 0, 0, 1, 0] |
| 。     | [0, 0, 0, 0, 1] |

今天 不错  -> [1, 0, 0, 1, 0]            今天 真 不错 -> [1, 0, 1, 1, 0]- 在对文本向量化时，也可以考虑词频，比如
不错  ->  [0, 0, 0, 1, 0]不错 不错 -> [0, 0, 0, 2, 0]- 有时也可以不事先准备词表，临时构建,如做文本比对任务，成对输入，此时维度可随时变化,比如：
    - 例1：         你好吗心情             A: 你好吗  ->  [1, 1, 1, 0, 0]        
    B: 你心情好吗 -> [1, 1, 1, 1, 1]      - 例2：          我不知道谁呀    A:我不知道 -> [1, 1, 1, 1, 0, 0]    B:谁知道呀 -> [0, 0, 1, 1, 1, 1]
    
##### one-hot编码缺点
- 如果有很多词，编码向量维度会很高，而且向量十分稀疏（大部分位置都是零），计算负担很大（维度灾难）- 编码向量不能反映字词之间的语义相似性，只能做到区分### 词向量 - word2vec我们希望得到一种词向量，使得向量关系能反映语义关系，比如：
- cos（你好， 您好） >  cos(你好，天气），即词义的相似性反映在向量的相似性。- 国王 - 男人 = 皇后 - 女人，即向量可以通过数值运算反映词之间的关系(类比关系)
- 同时，不管有多少词，向量维度应当是固定的#### Word embedding与Word vector的区别
- 本质上是一样的，都是以向量代表字符
- 一般说Word Embedding是指随机初始化的词向量或字向量- Word2Vec一般指一种训练Word Embedding的方法，使得其向量具有一定的性质（向量相似度反映语义相似度）#### Onehot编码 -> word vectors
将整个embedding矩阵看作一个线性层，Onehot 编码作为输入```math
\left[ 0,0,0,1,0 \right] * \begin{bmatrix} 17&24&1\\23&5&7\\4&6&13\\10&12&19\\11&18&25\\ \end{bmatrix} = \left[10,12,19\right]
```
```math
\left[ 0,0,0,1,0 \right] : 1 * 5的矩阵  
```
```math
\begin{bmatrix} 17&24&1\\23&5&7\\4&6&13\\10&12&19\\11&18&25\\ \end{bmatrix}: 5 * 3 的矩阵
```## 如何实现训练word embadding使得两个近义词可以实现向量夹角余弦值接近于1

### 基于语言模型
- **做出假设**：每段文本中的某一个词，由它前面n个词决定
    - 例如：    今天 天气 不错 我们 出去 玩    今天 -> 天气  -》 有一个模型可以由输入今天来输出结果天气    今天 天气 -> 不错  -》 与上面意思相同     今天 天气 不错 -> 我们    今天 天气 不错 我们 -> 出去 
- 论文参考：[《A Neural Probabilistic Language Model》- Bengio et al. 2003 ](./code/A_Neural_Probabilistic_Language_Model.pdf)

![](./image/1.png)

参考代码：[language_model.py](./code/language_model.py)

**PS**:相关论文资源可在[https://arxiv.org](https://arxiv.org)上进行搜索

### 基于窗口
- **做出假设**：如果两个词在文本中出现时，它的前后出现的词相似，则这两个词语义相似。比如    - 你想<u>明白</u>了吗 / 你想<u>清楚</u>了吗 ->  窗口长度 = 5, 目标词=明白/清楚       - 今天<u>北京</u>很热 /今天<u>南京</u>很热 -> 窗口长度 = 3,目标词=北京/南京#### 如何训练

##### CBOW模型：:基于前述思想，我们尝试用窗口中的词（或者说周围词）来表示（预测）中间词

- 示例：
    - 窗口：你想<u>明白</u>了吗    - 输入：你/想/了/吗    - 输出：明白
![CBOW模型](./image/2.png)

假设我们的语言里只有这四个单词
![](./image/3.png)
![](./image/4.png)
![](./image/5.png)
![](./image/6.png)
![](./image/7.png)


##### SkipGram模型 -- 用中间词来表示周围词- 示例：
    - 窗口：你想<u>明白</u>了吗
    - (输入,输出)：（明白,你）/（明白,想）/（明白,了）/（明白,吗） ![](./image/8.png)

- 面临的问题
    - 1.输出层使用one-hot向量会面临维度灾难，因为词表可能很大。    - 2.收敛速度缓慢